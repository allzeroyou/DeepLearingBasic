# 중간고사
4월 27일 수요일 대면


신경망 학습에 대한 내용을 다룹니다.
학습목표는 손실함수의 값을 적게 만드는 `경사법`에 대해 이해한다.

# 데이터 학습

**학습**이란,
훈련 데이터를 이용해 `매개변수`인 `가중치의 최적값`을 자동으로 구하는 것을 의미.

신경망이 학습할 때, 손실 함수를 지표로 하는데
이때, 손실 함수의 결과값을 **가장 작게 하는** 가중치를 구하는 것이 학습의 목표이다.

![](https://velog.velcdn.com/images/allzeroyou/post/6c0698a9-2969-4755-9819-4c28d5927d17/image.png)

기존에는 데이터가 주어지면 사람이 모든 규칙을 만들거나 특징을 설계 => 신경망은 데이터로부터 스스로 학습해 규칙을 찾아냄.

이러한 이유로 딥러닝을 종단간 기계학습(end-to-end learning)이라고도 함.
`처음부터 끝까지` 데이터 입력에서 결과 출력까지 사람의 개입없이 얻음을 뜻함.

## 훈련 데이터와 시험 데이터

머신 러닝은 데이터를 훈련데이터, 시험데이터로 나눠 학습과 실험을 수행.
훈련 데이터만을 이용해 학습을 진행하며 최적의 `매개변수` 인 `가중치와 편향` 찾기

시험 데이터를 사용해 훈련된 모델의 성능을 평가

이때 시험 데이터를 사용하는 이유는 `범용 능력`을 평가함을 위함. 다뤘던 문제인 훈련 데이터에만 성능이 높고 새로운 데이터에 대해서 문제가 있으면 안되고 말 그대로 '범용적인', _universal_하게 성능을 보이길 원함.

한 데이터셋에만 지나치게 최적화된 상태를 `오버피팅`(overfitting)이라고 한다.



# 손실 함수

손실함수는 측정한 데이터를 토대로 산출한 모델의 예측값(y)과 실제값, 정답(t)의 차이를 표현하는 지표로, 모델이 _얼마나 데이터를 잘 표현하지 못하는가_ 를 나타낸다.

손실함수는 정답과 예측을 입력으로 받아 실숫값 지표를 만드는데, 이 지표의 숫자가 _높을 수록_ 모델의 성능이 좋지 않은것.

따라서 손실함수의 함숫값이 **최소**가 되도록 `가중치`와 `편향`을 찾는 것이 딥러닝 학습의 목표.

➡️ `예측값`과 `실제값`에 대한 `오차`를 줄이기 위함.

## 평균제곱오차(Mean Square Error, MSE)
![](https://velog.velcdn.com/images/allzeroyou/post/66754235-dad3-46e2-a67e-b84626605b46/image.png)

- y(예측값)과 t(실제값, 정답) 사이의 평균제곱오차를 정의.
- 공식이 매우 간단
- 차가 커질수록, 제곱 연산으로 인해 값이 더욱 뚜렷해짐
- 제곱으로 인해 오차가 양수이든 음수이든 누적 값 증가시킴

![](https://velog.velcdn.com/images/allzeroyou/post/9d79d7b5-6a67-4add-933a-6b7829ff765d/image.png)
`y`는 신경망 출력층의 결과인 소프트맥스 함수의 출력값
`t`는 one-hot encoding으로 표현된 정답값

> one-hot-encoding
레이블 중 오직 하나만 HOT하고, 나머지는 COLD한 데이터.
고유 값에 해당하는(hot) 컬럼에만 1을 표시, 나머지 컬럼(cold)에는 0을 표시

### 손글씨 숫자 인식에서 손실함수(평균 제곱 오차) 구하기

```python
import numpy as np

# 손실함수 정의
def sum_square_error(y, t):
  return 0.5 * np.sum((y-t)**2)

y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]


print(sum_square_error(np.array(y), np.array(t))) 
# 0.09750000000000003
# 손실 함수 값 낮음 => 성능 좋음(정답, 잘 추정된 값)


y = [0.1, 0.05, 0.1, 0.0, 0.7, 0.3, 0.0, 0.0, 0.0, 0.0]

print(sum_square_error(np.array(y), np.array(t)))
# 0.7012499999999999
# 손실 함수 값 높음 => 성능 안 좋음(오답, 잘못 추정된 값)
```


## 교차엔트로피오차
![](https://velog.velcdn.com/images/allzeroyou/post/3b12eefc-e5b4-4750-8276-2c741984773f/image.png)

`엔트로피`는 물리학상 어지러운 정도를 뜻하되, 신경망에서는 정보의 양을 뜻하며 신호인식에 쓰인다.

이때, 정보는 신호에 존재하는 정보의 양이며 누구나 알만한 정보가 아닌 새롭고 특이해 사람들을 놀라게 만드는 정도가 클수록 많다.
ex. 나 내일 학교가 => 누구나 예측가능만 정보임. 정보의 양 적음
나 내일 우주선 타 => 예측하기 어려운 새로운 정보임. 정보의 양 많음.

- 정보엔트로피: 하나의 확률분포가 갖는 불확실성(놀람의 정도)혹은 정보량을 정량적으로 계산할 수 있도록 하는 개념이다.

- 교차엔트로피: 두 가지 확률분포가 얼마나 비슷한지, 수리적으로 나타내는 개념.
![](https://velog.velcdn.com/images/allzeroyou/post/c27dbc2f-7f00-4546-89d6-40fefb654991/image.png)

실제 분포 q에 대해 알지못하는 상태에서 모델링을 통해 구한 분포인 p를 통해 q를 예측함.

이때 q와 p가 모두 들어가기 때문에 '교차'엔트로피라고 함

- q는 딥러닝 모델의 추정 확률 분포
- p는 딥러닝 모델이 추구해야할 미지의 확률 분포

교차엔트로피에서는 실제값과 예측값이 맞는 경우에 0으로 수렴, 값이 틀릴 경우 발산
따라서, 두 확률분포가 **서로 얼마나 다른지 나타내는 정량적 지표 역할**수행.

목적은 단 하나, _예측값과 실제값의 차이를 줄이기 위해_ 사용됨.

![](https://velog.velcdn.com/images/allzeroyou/post/3b12eefc-e5b4-4750-8276-2c741984773f/image.png)
교차 엔트로피오차 수식에서 log는 밑이 e인 자연로그.
t는 one-hot encoding으로 표현된 정답 레이블이므로, 결국 실제로 정답일 때(t=1) softmax 예측값에 대한 자연로그를 계산하는 식.

ex. 정답이 2이고, 신경망의 softmax 출력 값이 0.6 => 교차엔트로피오차는 -log0.6=0.51이 된다.(왜?)

같은 조건에서 신경망의 출력값이 0.1 => -log0.1=2.30

결론적으로, 교차 엔트로피오차는 정답일때 신경망 출력 값이 전체 값이 된다.

y=logx

![](https://velog.velcdn.com/images/allzeroyou/post/83f18b99-f0eb-40ea-818b-1bbfcad92b22/image.png)

- x=1일때 y=0
- x가 0에 가까워질때 y는 작아짐.

**즉, 예측값이 커질수록 오차는 0에 가까워지고, 예측 값이 작아질수록 오차는 커진다.**
![](https://velog.velcdn.com/images/allzeroyou/post/2c44e0d0-9d31-4671-8b6d-47f23f4c3ea2/image.png)


```python
from cmath import e
import numpy as np
import math

def cross_entropy_error(y, t):
  delta = 1e-7
  return -np.sum(t * np.log(y + delta))
  # 예측값 y에 아주 작은 값인 delta를 더함

# 잘 추정된 값
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
# 잘못 추정된 값
y2 = [0.1, 0.05, 0.3, 0.0, 0.25, 0.6, 0.0, 0.1, 0.0, 0.0]
# 정답 레이블
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

print(cross_entropy_error(np.array(y), np.array(t))) # 0.510825457099338
print(cross_entropy_error(np.array(y2), np.array(t))) # 1.2039724709926583
```
평균제곱오차와 마찬가지로 교차엔트로피 오차에서도 **더 작은** 값이 출력되는 y가 정답에 가까운 추정값이다.



## 미니배치학습
손실 함수는 훈련 데이터가 100개 있다면 그 100개의 손실 함수의 값들의 합으로 이뤄진다.

즉, 손실함수는 모든 훈련 데이터를 대상으로 구해야 한다.
![](https://velog.velcdn.com/images/allzeroyou/post/95ba885a-9e98-474f-82f9-e3b6396d3def/image.png)

그러나 데이터의 개수가 많으면 모든 데이터를 학습하는데 시간이 오래 걸린다.
따라서 전체 데이터 중 일부를 `랜덤`으로 선별해 근사치 값을 구한다.
`선별된 일부 데이터`를 `미니배치(mini-batch)`라고 한다

```python
# 미니 배치 학습을 위한 준비
import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=True)

print(x_train.shape) # (60000, 784)
print(t_train.shape) # (60000, 10)

# 훈련데이터에서 무작위로 10장 뽑기
train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]

print(np.random.choice(60000, 10)) # [50675 57384 51203 13246 33019  1454 49923 32680 42774   545]
```

### 배치용 교차엔트로피오차 구현하기
1) one-hot-encoding
```python
# one-hot-encoding
def cross_entropy_error(y ,t):
  if y.ndim == 1:
    t = t.reshape(1, t.size)
    y = y.reshape(1, y.size)

  batch_size = y.shape[0]
  return -np.sum(t * np.log(y + 1e-7)) / batch_size
```

2) 숫자 레이블
```python
# number label
def cross_entropy_error(y ,t):
  if y.ndim == 1:
    t = t.reshape(1, t.size)
    y = y.reshape(1, y.size)

  batch_size = y.shape[0]
  return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
```
## 왜 정확도 대신 손실 함수?
적절한 매개변수를 찾기위한 지표로 정확도를 사용하지 않고 손실함수를 설정하는 이유는,

신경망 학습에서의 `미분` 역할에 있다.
신경망 학습에서 최적의 매개변수를 탐색할 때 매개변수에 대해 미분(기울기)을 계산하고 미분 값을 갱신하는 과정을 반복 => 손실 함수의 값을 작게 하는 매개변수를 find!

이때 미분 값이 양수이면 매개변수를 음의 방향으로 갱신,
미분 값이 음수이면 매개변수를 양의 방향으로 갱신.

반면, 정확도를 지표로 삼으면 대부분의 미분값이 0이라 갱신불가.
ex. 100장의 데이터 중 30장을 제대로 인식한다고 가정하자.
이때 정확도는 30%
매개변수의 값을 미세하게 조정 => 정확도가 개선되지 않고 일정할 것.
왜?
30, 31, 32%와 같이 불연속적으로 개선되고 30.01444%처럼 조정되지 않기 때문이다.

**즉, 손실함수는 매개변수의 값이 조금 변하면 그에 연속적인 값으로 반응하나 정확도는 거의 반응을 보이지 않는다.**
이렇나 성질은 계단 함수를 활성화함수로 사용하지 않는 이유와 일맥상통.



# 수치 미분
# 기울기
# 학습 알고리즘 구현
